diff --git a/data_processor.py b/data_processor.py
index 1234567..abcdefg 100644
--- a/data_processor.py
+++ b/data_processor.py
@@ -1,15 +1,42 @@
 import os
 import json
 import logging
+from typing import Dict, List, Any, Optional, Union
+from datetime import datetime
 
 logger = logging.getLogger(__name__)
 
-def process_data(input_file, output_file):
+def process_data(input_file: str, output_file: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
     """
-    Process data from input file and save to output file
+    Process data from input file and save to output file.
+    
+    Args:
+        input_file: Path to the input JSON file
+        output_file: Path to save the processed output
+        config: Optional configuration dictionary with processing options
+        
+    Returns:
+        Dictionary containing processing statistics
     """
+    start_time = datetime.now()
+    
+    # Set default configuration if not provided
+    if config is None:
+        config = {
+            "normalize": True,
+            "remove_duplicates": True,
+            "max_items": None
+        }
+    
     logger.info(f"Processing data from {input_file} to {output_file}")
     
+    # Validate input file
+    if not os.path.exists(input_file):
+        error_msg = f"Input file not found: {input_file}"
+        logger.error(error_msg)
+        raise FileNotFoundError(error_msg)
+    
+    # Load data
     with open(input_file, 'r') as f:
         data = json.load(f)
     
@@ -17,9 +44,44 @@ def process_data(input_file, output_file):
     processed_data = []
     for item in data:
         processed_item = {
-            'id': item['id'],
-            'value': item['value'] * 2
+            'id': str(item.get('id', '')),
+            'value': float(item.get('value', 0)) * 2,
+            'processed_at': datetime.now().isoformat()
         }
         processed_data.append(processed_item)
     
-    # Save processed data
+    # Apply normalization if configured
+    if config.get("normalize", False):
+        processed_data = _normalize_data(processed_data)
+    
+    # Remove duplicates if configured
+    if config.get("remove_duplicates", False):
+        processed_data = _remove_duplicates(processed_data)
+    
+    # Apply max items limit if configured
+    max_items = config.get("max_items")
+    if max_items is not None and isinstance(max_items, int):
+        processed_data = processed_data[:max_items]
+    
+    # Save processed data
+    with open(output_file, 'w') as f:
+        json.dump(processed_data, f, indent=2)
+    
+    # Generate statistics
+    end_time = datetime.now()
+    stats = {
+        "input_items": len(data),
+        "output_items": len(processed_data),
+        "processing_time_ms": (end_time - start_time).total_seconds() * 1000,
+        "output_file": output_file
+    }
+    
+    logger.info(f"Processing completed: {stats}")
+    return stats
+
+def _normalize_data(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
+    """Normalize values in the data to be between 0 and 1"""
+    if not data:
+        return data
+        
+    # Find maximum value for normalization
+    max_value = max(item.get('value', 0) for item in data)
+    if max_value == 0:
+        return data
+        
+    # Normalize values
+    for item in data:
+        if 'value' in item:
+            item['value'] = item['value'] / max_value
+            
+    return data
+
+def _remove_duplicates(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
+    """Remove duplicate items based on ID"""
+    seen_ids = set()
+    unique_data = []
+    
+    for item in data:
+        item_id = item.get('id')
+        if item_id not in seen_ids:
+            seen_ids.add(item_id)
+            unique_data.append(item)
+            
+    return unique_data
